\section{Setting a Baseline}
\subsection{Required submission files}
\begin{enumerate}
	\item \hl{The updated gauss.c.}

	\item \hl{The performance plots and description in the report.}

\end{enumerate}

\subsection{Questions}
\begin{enumerate}
	\item \hl{Which patterns were identified and replaced with collective communication in the code? Explain.}
	There were several regions that could be replaced with collective communication:
	\begin{enumerate}
	\item 1) The fan-out when data is sent from the root process to all other processes. This could be replaced by a simple \verb!MPI_Bcast! operation. 
	
	\item 2) The sending of pivot entries to each other process after performing the elimination and normalization step. The sending and the calculation steps were done completely sequentially---the process with the current pivot entry would finish the gaussian ellimination steps on the entire local block before any sending took place. We interleaved \verb!MPI_Ibcast! collective operations such that each row would be sent to each other process via a non-blocking collective broadcast once computation on it was complete.
	
	\item 3) The sending of segments of the processed solution vector to perform a "triangular solve". We used a series of \verb!MPI_Bcast! operations to send this information instead of \verb!MPI_Send! operations in for loops
	
	\item 4) The fan-in of the solution vector to the root process. We used an \verb!MPI_Gather! to collect the solution vector into the \verb!solution! array.
	
	\end{enumerate}
	\item \hl{Were you able to identify any potential for overlap and used any non-blocking collectives? (Use Vampir)}	
	Theoretical overlap can be achieved during the sending of rows during the computation steps. We implemented the non-blocking collective \verb!MPI_Ibcast! once each row was ready to be sent. Each other process would start the corresponding \verb!MPI_Ibcast! operation for all rows from the process with the pivot row; an \verb!MPI_Wait! function would be called before the exact pivot line was used for calculation.
	
	Theoretically, this would result in calculation/communication overlap, as new rows would be received as calculation using previous rows was taking place.
	
	
	\item \hl{Was there any measurable performance or scalability improvement as a result of these changes?}		
	
	\item \hl{Is the resulting code easier to understand and maintain after the changes? Why?}	
\end{enumerate}
	Certain segments of the code are easier to understand, while certain segments of the code are more complicated. We have replaced large for loops with collective operations---this reduces \verb!MPI_Send!/\verb!MPI_Recv! calls with a single collective call. The code is then less dense and it is easier to see how data is distributed among the various process ranks.
	
	However, we are also employing custom \verb!MPI_Group!s/\verb!MPI_Comm!s to control the sets of processes that are involved in each collective operation. This is a more advanced use of MPI functions and basic users may be confused. 
	
	Overall, the code is easier to maintain, in the sense that the individual chunks are easier to read. However, the cost of collective operations is the lack of flexibility---adapting the code for additional functionality or for greater generality will be harder as we have replaced all p2p communications with collective operations, ie. this may need to be reverted if the algorithm is changed.




% % Figure example
% \begin{figure}[p] % h=here, t=top, b=bottom, p=(extra)page, !=force
%  	\begin{center}
%  		\includegraphics[width=.9\linewidth]{figure.png} % It searches in the Figures/ folder!
%  		\caption{Caption text}
%  		\label{fig:figureLabelName}
%  	\end{center}
% \end{figure}
