\section{Setting a Baseline}
\subsection{Required submission files}
\begin{enumerate}
	\item \hl{The updated Load-Leveler batch script.}

		\verb!Data/Baseline/updated_job.ll!

	\item \hl{The performance plots and description in the report.}

		Figures \ref{fig:IO}, \ref{fig:setup}, \ref{fig:total}, \ref{fig:varying_size}, \ref{fig:domain}, and \ref{fig:mpi_b} show the various performance plots for the baseline.

\end{enumerate}

\subsection{Questions}
\begin{enumerate}
	\item \hl{Briefly describe the Gaussian Elimination and the provided implementation.}

	The Gaussian Elimination (GE) is a program that converts a dense matrix into an upper-right triangular matrix. This is done via a recursive algorithm that sets entire columns of the matrix to zero by:
\begin{enumerate}
\item Choosing a pivot. This is typically the value in the diagonal. Pivoting may be performed to shift the row with the largest value in that column into the correct position
\item The factor $\frac{a_{ij}}{a_{ii}}$ is calculated for each row in the column that is below the diagonal, such that $i > j$ if the topmost leftmost matrix entry is set as $a_{00}$.
\item All rows in the column below the diagonal are set to zero using the calculated factor
\end{enumerate}

The provided implementation uses segments the matrix into rows and provides a set of rows to each process. The value of pivots are sent to each processor in a sequential, cascading manner. Each row then calculates the factor mentioned in the algorithm and then performs the linear operations to set the correct column values to 0.

Gaussian Elmination code presents a major challenge in load-balancing---as the algorithm progresses, the amount of work available for each process decreases. When the last few rows are undergoing GE, all but one process is still performing work. 

	\item \hl{How is data distributed among the processes?}

	Each process takes a number of rows of the matrix equal to $\frac{row}{\#processes}$. There are checks in place such that the number of processes must be able to fully divide the total number of rows.

	\item \hl{Explain the changes applied to the provided Load-Leveler batch script.}

	One of the main difficulties is getting a good average value for the times. As such, the main way to reduce variance in results is to run each instance multiple times. In our case, we ran the code 5 times for each combination of MPI processes and domain size.

	\item \hl{What were the challenges in getting an accurate baseline time for Gaussian Elimination.}

	The main challenges were to determine what was important when determining a baseline. The Gaussian When improving the aglorithm, the main focus will be to reduce the amount of sequential communication---so that each process is able to receive updated information with as little wait time as possible. Thus, our chosen baseline should show improvement when the communication algorithm is improved. The following show the respective baselines chosen for each of the measured times:
\begin{enumerate}
	\item I/O Time

	I/O is only performed on the main process or process 0. Thus, it only makes sense to note this from process 0. Fig. shows the average time for IO operations in process 0 for the various domain sizes. Since the IO operation takes place completely sequentially, the number of processes does not affect the times. We see the average IO times for each domain size in the Fig. \ref{fig:IO}.
	
	
% % Figure example
 \begin{figure}[p] % h=here, t=top, b=bottom, p=(extra)page, !=force
 	\begin{center}
  		\includegraphics[width=.9\linewidth]{Baseline/IO.png} % It searches in the Figures/ folder!
  		\caption{IO Time vs. Domain Size}
  		\label{fig:IO}
  	\end{center}
 \end{figure}

	\item Setup Time

	The setup time measures the amount of time each process waits before they fully receive their local matrix rows and local rows of the RHS vector. Since the sending process from process 0 is sequential---process 0 uses blocking communication to send data to process 1, 2, 3 ... etc, some processes with higher rank will wait for longer. Improvement will then be indicated by the \textit{longest} wait time of all processes. We see that the setup time scales with the total domain size---which make sense as a larger domain necessitates more data sent from the root process to each other process. This can be seen for any number of processes, as shown in Fig. \ref{fig:setup}.
	
	
	\begin{figure}[p] % h=here, t=top, b=bottom, p=(extra)page, !=force
		\hspace*{-0.25\linewidth}\begin{tabular}{cc}
			\includegraphics[width=.7\linewidth]{Baseline/setup_process8.png} & \includegraphics[width=.7\linewidth]{Baseline/setup_process32.png} \\
		    (a) 8 Processes & (b) 32 Processes\\[6pt]\\
		\end{tabular}
		\caption{Setup Time vs Domain Size.}
		\label{fig:setup}
	\end{figure}
	
	\item Compute Time
	
	The compute time measures the amount of time between the start of the gaussian elimination steps to the end of the gaussian elimination steps. This includes the MPI time. Therefore, to determine the actual compute time, we subtract the MPI time from the compute time and average the results across the multiple processes. This is because the overall computational work for each domain size should remain relatively constant---regardless of the number of processes used. If an algorithmic improvement occurs in the code, this should improve this benchmark.
	
	\item MPI Time
	
	The MPI time sums up the total amount of time spent during communication. We hope to decrease this value by improving the communication and computation overlap. The MPI time increases with increased domain size as well and generally decreases with an increased number of nodes (as the amount sent to each node decreases). However, for very small domain sizes, we see the cost of communication overtake the benefits of multiple nodes. This is seen for domain sizes of 64x64 with 64 processes.
	
	\item Total Time
	
	The total time indicates the amount of time required to fully execute the entire code. We see that, for larger domain sizes, a larger number of processes reduces total exeuction time. However, for smaller domain sizes, there comes a point where the costs of communication outweigh the benefits. This can be seen in Fig. \ref{fig:total}
	
	 \begin{figure}[p] % h=here, t=top, b=bottom, p=(extra)page, !=force
 		\begin{center}
  		\includegraphics[width=.9\linewidth]{Baseline/total_multdomain.png} % It searches in the Figures/ folder!
  		\caption{Total Time vs. \#Processes}
  		\label{fig:total}
 	 	\end{center}
 	\end{figure}
	\end{enumerate}


	\item \hl{Describe the compute and MPI times scalability with fixed process counts and varying size of input files for the Sandy Bridge and Haswell nodes. Did you observe any differences?}
	
	
	\begin{figure}[p] % h=here, t=top, b=bottom, p=(extra)page, !=force
		\hspace*{-0.25\linewidth}\begin{tabular}{cc}
			\includegraphics[width=.7\linewidth]{Baseline/compute_multprocess.png} & \includegraphics[width=.7\linewidth]{Baseline/mpi_multproc.png} \\
		    (a) Compute Time vs. Domain Size & (b) MPI Time vs. Domain Size\\[6pt]\\
		\end{tabular}
		\caption{Fixed Process Counts \& Varying Size.}
		\label{fig:varying_size}
	\end{figure}
	
	With fixed process count and increasing the domain size, we see that the compute time increases with domain size, as shown in Fig \ref{fig:varying_size}(a). This is not surprising as there is more time required for computation. We see in Fig. \ref{fig:varying_size}(a) that this remains true for any number of processes. However, we see that while the Haswell nodes are faster at a 8 processes, the Sandy Bridge nodes are faster when the number of nodes is increased to 64. This seems to indicate that Sandy Bridge processors scale better with a larger number of processes.

We see that the MPI times follow a similar pattern---with a larger domain size, the amount of time in communication increases. This remains true at any number of processes, as shown in Fig. \ref{fig:varying_size}(b). The shape of the graphs are very similar to that seen for the compute time. We also see similar behavior that the Sandy Bridge processors scale better with a larger number of processes.


	\item \hl{Describe the compute and MPI times scalability with fixed input sets and varying process counts for the Sandy Bridge and Haswell nodes. Did you observe any differences?}

	\begin{figure}[p] % h=here, t=top, b=bottom, p=(extra)page, !=force
		\hspace*{-0.25\linewidth}\begin{tabular}{cc}
			\includegraphics[width=.7\linewidth]{Baseline/compute_domain64.png} & \includegraphics[width=.7\linewidth]{Baseline/compute_domain8192.png} \\
			(a) Compute Time vs. \#Processors &  Time vs. \#Processors\\[6pt]
		\end{tabular}
		\caption{Compute Time vs \#Processes.}
		\label{fig:domain}
	\end{figure}


With fixed domain size and increasing the process count, we see that the compute time decreases with process count. This is not surprising because each process works on a smaller segment of the full matrix. We see in Fig. \ref{fig:domain} that this remains true for any domain size. For small domain sizes, such as 64x64, the compute time is completely negligible in comparison to the communication time. For larger domain sizes, the compute time becomes much more significant. We see that the Haswell nodes remain faster for any processor when the domain size is small. Once the domain size exceeds a certain threshold, the Sandy Bridge nodes are seen to scale better with a larger number of processes.

	 \begin{figure}[p] % h=here, t=top, b=bottom, p=(extra)page, !=force
 		\begin{center}
  		\includegraphics[width=.9\linewidth]{Baseline/mpi_multdomain.png} % It searches in the Figures/ folder!
  		\caption{MPI Time vs. \#Processes}
  		\label{fig:mpi_b}
 	 	\end{center}
 	\end{figure}

We expect to see a longer MPI time as the number of processes increase. However, this does not seem to be the case across the board. This is true for the smaller domain sizes, but not the larger domain sizes, as seen in Fig \ref{fig:mpi_b}. This is somewhat surprising as we would expect the cost of comnunication to increase with increased processes. We attribute this observed effect to the sequential nature of the MPI passing---the first process waits for all other processes to finish computing before it returns. As a result, by reducing overall computational time, we decrease the time spent waiting for blocking MPI calls.

We see that the Sandy Bridge architecture performs better overall---at very small process counts it is, overall, slightly faster than the Haswell architecture. At larger domain sizes, the difference in MPI times very large, but shrinks with an increasing number of processors.

\end{enumerate}

% % Figure example
% \begin{figure}[p] % h=here, t=top, b=bottom, p=(extra)page, !=force
%  	\begin{center}
%  		\includegraphics[width=.9\linewidth]{figure.png} % It searches in the Figures/ folder!
%  		\caption{Caption text}
%  		\label{fig:figureLabelName}
%  	\end{center}
% \end{figure}
